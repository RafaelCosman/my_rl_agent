{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Workout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "NOW\n",
    "- Try the simann approach\n",
    "\n",
    "LATER\n",
    "- Penalize the env for long-term differences\n",
    "\n",
    "DONE\n",
    "- Make the agent training stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import random\n",
    "import datetime\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use(\"Pdf\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the session. An interactive session is a session that is automatically your default session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own little library of helper functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.569  0.551]]\n",
      "[[ 0.069  0.092]]\n"
     ]
    }
   ],
   "source": [
    "def fc(input_tensor, in_size, out_size, collection=None, non_linearity=tf.nn.sigmoid):\n",
    "    \n",
    "    collections = [tf.GraphKeys.VARIABLES, tf.GraphKeys.TRAINABLE_VARIABLES]\n",
    "    if collection is not None:\n",
    "        collections.append(collection)\n",
    "        \n",
    "    W = tf.get_variable(\n",
    "        name=\"W\",\n",
    "        initializer=tf.truncated_normal(\n",
    "            shape=[in_size, out_size],\n",
    "            mean=0.0,\n",
    "            stddev=0.1\n",
    "        ),\n",
    "        collections=collections\n",
    "    )\n",
    "    \n",
    "    b = tf.get_variable(\n",
    "        name=\"b\",\n",
    "        initializer=tf.constant(\n",
    "            value=0.1,\n",
    "            shape=[out_size]\n",
    "        ),\n",
    "        collections=collections\n",
    "    )\n",
    "    \n",
    "    return non_linearity( tf.matmul(input_tensor, W) + b )\n",
    "\n",
    "# fc test\n",
    "fc_test_template = tf.make_template(\"fc_test_template\", fc, in_size=2, out_size=2)\n",
    "op = fc_test_template([[0., 1.]])\n",
    "tf.initialize_all_variables().run()\n",
    "print op.eval()\n",
    "\n",
    "def fc_stack(input_tensor, list_of_sizes, collection=None):\n",
    "    result = input_tensor\n",
    "    \n",
    "    for layer_index in xrange(len(list_of_sizes)-1):\n",
    "\n",
    "        with tf.variable_scope(\"layer\"+str(layer_index)):\n",
    "        \n",
    "            if layer_index == len(list_of_sizes)-2:\n",
    "                non_linearity = tf.identity\n",
    "            else:\n",
    "                non_linearity = tf.nn.relu\n",
    "        \n",
    "            result = fc(\n",
    "                result,        \n",
    "                in_size=list_of_sizes[layer_index],\n",
    "                out_size=list_of_sizes[layer_index+1],\n",
    "                collection=collection,\n",
    "                non_linearity=non_linearity\n",
    "            )\n",
    "        \n",
    "    return result\n",
    "\n",
    "# fc_stack test\n",
    "fc_stack_test_template = tf.make_template(\"fc_stack_test_template\", fc_stack, list_of_sizes=[2, 3, 2])\n",
    "op = fc_stack_test_template([[0., 1.]])\n",
    "tf.initialize_all_variables().run()\n",
    "print op.eval()\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def one_hot(index, name=\"one_hot\"):\n",
    "    return tf.one_hot(indices=index, depth=env.observation_space.n, on_value=1, off_value=0, axis=None, name=name).eval()\n",
    "\n",
    "def getFeedDict(observation):\n",
    "    if type(env.observation_space) == gym.spaces.discrete.Discrete:\n",
    "        return {x:[one_hot(observation)]}\n",
    "    elif type(env.observation_space) == gym.spaces.box.Box:\n",
    "        return {x:[observation]}\n",
    "    else:\n",
    "        print \"ERR\"\n",
    "        \n",
    "class InitializeNewVariables:\n",
    "    def __enter__(self):\n",
    "        self.temp = set(tf.all_variables())\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        sess.run(tf.initialize_variables(set(tf.all_variables()) - self.temp))\n",
    "\n",
    "        \n",
    "### OpenAI Gym specific stuff ###\n",
    "def bound_to_env_action_space(my_action):\n",
    "    my_action = tf.maximum(my_action, env.action_space.low)\n",
    "    my_action = tf.minimum(my_action, env.action_space.high)\n",
    "    return my_action\n",
    "    \n",
    "def bound_to_env_observation_space(my_observation):\n",
    "    my_observation = tf.maximum(my_observation, env.observation_space.low)\n",
    "    my_observation = tf.minimum(my_observation, env.observation_space.high)\n",
    "    return my_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-06-09 16:22:51,504] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(1,)\n",
      "Box(3,)\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('ConfirmationBiasEasy-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('TwoRoundNondeterministicReward-v0')\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "print(env.action_space)\n",
    "ACTION_DIMS = env.action_space.shape[0]\n",
    "print(env.observation_space)\n",
    "OBSERVATION_DIMS = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some random data with which to build our env model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_observations = []\n",
    "actions = []\n",
    "observations = []\n",
    "rewards = []\n",
    "dones = []\n",
    "initial_observations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the actual simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_data(epsilon=0.0, num_episodes=2, episode_len=200, render=True, feed_dict={}):\n",
    "    total_reward = 0\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        observation = observation.astype(np.float32)\n",
    "        initial_observations.append(observation)\n",
    "\n",
    "\n",
    "        for timestep in xrange(episode_len):\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample() #choose a random action\n",
    "            else:\n",
    "                feed_dict[agent_in] = np.reshape(observation, [1, 3])\n",
    "                action = bound_to_env_action_space(agent_out).eval(feed_dict)[0]\n",
    "\n",
    "            old_observation = observation\n",
    "\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            observation = observation.astype(np.float32)\n",
    "\n",
    "            old_observations.append(old_observation)\n",
    "            actions.append(action)\n",
    "            observations.append(observation)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(timestep+1))\n",
    "                break\n",
    "\n",
    "    print \"We now have \" + str(len(observations)) + \" observations in total.\"\n",
    "    \n",
    "    return total_reward / (num_episodes * episode_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 20000 observations in total.\n"
     ]
    }
   ],
   "source": [
    "generate_data(1.0, num_episodes=100, render=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we make the env model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_model_predicted_outs: Tensor(\"env_model/env_model/layer1/Identity:0\", shape=(?, 5), dtype=float32)\n",
      "env_model_loss: Tensor(\"env_model/env_model_loss:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ENV_MODEL_OUT_DIMS = OBSERVATION_DIMS + 1 + 1\n",
    "ENV_MODEL_IN_DIMS = OBSERVATION_DIMS + ACTION_DIMS\n",
    "\n",
    "env_model = tf.make_template(\n",
    "    \"env_model\",\n",
    "    fc_stack,\n",
    "    list_of_sizes=[ENV_MODEL_IN_DIMS, 1000, ENV_MODEL_OUT_DIMS],\n",
    "    collection=\"env_model_vars\"\n",
    ")\n",
    "\n",
    "env_model_predicted_observation = tf.make_template(\n",
    "    'env_model_predicted_observation',\n",
    "    lambda env_model_predicted_outs: tf.slice(\n",
    "        env_model_predicted_outs,\n",
    "        begin=[0, 0],\n",
    "        size=[-1, OBSERVATION_DIMS],\n",
    "        name=\"env_model_predicted_observation\"\n",
    "    )\n",
    ")\n",
    "\n",
    "env_model_predicted_reward = tf.make_template(\n",
    "    'env_model_predicted_reward',\n",
    "    lambda env_model_predicted_outs: tf.slice(\n",
    "        env_model_predicted_outs,\n",
    "        begin=[0, OBSERVATION_DIMS],\n",
    "        size=[-1, 1],\n",
    "        name=\"env_model_predicted_reward\"\n",
    "    )    \n",
    ")\n",
    "\n",
    "env_model_predicted_done = tf.make_template(\n",
    "    'env_model_predicted_done',\n",
    "    lambda env_model_predicted_outs: tf.slice(\n",
    "        env_model_predicted_outs,\n",
    "        begin=[0, OBSERVATION_DIMS + 1],\n",
    "        size=[-1, 1],\n",
    "        name=\"env_model_predicted_done\"\n",
    "    )\n",
    ")\n",
    "\n",
    "with tf.name_scope('env_model'):\n",
    "    env_model_ins = tf.placeholder(dtype=tf.float32, shape=[None, ENV_MODEL_IN_DIMS], name='ins')\n",
    "    env_model_outs = tf.placeholder(dtype=tf.float32, shape=[None, ENV_MODEL_OUT_DIMS], name='outs')\n",
    "    env_model_predicted_outs = env_model(env_model_ins)\n",
    "    print \"env_model_predicted_outs:\", env_model_predicted_outs\n",
    "\n",
    "    env_model_loss = tf.reduce_mean(tf.square(tf.sub(env_model_predicted_outs, env_model_outs)), name=\"env_model_loss\")\n",
    "    print \"env_model_loss:\", env_model_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = tf.make_template(\n",
    "    \"agent\",\n",
    "    fc_stack,\n",
    "    list_of_sizes=[3, 100, 1],\n",
    "    collection=\"agent_vars\"\n",
    ")\n",
    "\n",
    "agent_in = tf.placeholder(dtype=tf.float32, shape=[None, OBSERVATION_DIMS])\n",
    "agent_out = agent(agent_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make giant stack of agent, env, agent, env, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intermediate_env_rewards = []\n",
    "intermediate_actions = []\n",
    "intermediate_observations = []\n",
    "\n",
    "with InitializeNewVariables():\n",
    "    prob_not_done = tf.Variable(tf.constant(1.), trainable=False, name=\"prob_not_done\")\n",
    "    total_predicted_reward = tf.Variable(tf.constant(0.), trainable=False, name=\"total_predicted_reward\")\n",
    "\n",
    "predicted_observations = tf.placeholder_with_default(\n",
    "    np.asarray(initial_observations),\n",
    "    [None, 3],\n",
    "    name=\"initial_predicted_observations\"\n",
    ")\n",
    "\n",
    "EPISODE_LENGTH = 200\n",
    "\n",
    "for round in xrange(EPISODE_LENGTH):\n",
    "    action = bound_to_env_action_space(agent(predicted_observations))\n",
    "    \n",
    "    predicted_outs = env_model(tf.concat(1, [predicted_observations, action]))\n",
    "    \n",
    "#     rewardFromThisRound = tf.mul(envModelPredictedReward(predictedOuts), probNotDone)\n",
    "    reward_from_this_round = env_model_predicted_reward(predicted_outs)\n",
    "    \n",
    "    intermediate_env_rewards.append(reward_from_this_round)\n",
    "    intermediate_actions.append(action)\n",
    "    intermediate_observations.append(predicted_observations)\n",
    "    \n",
    "    total_predicted_reward = tf.add(total_predicted_reward, reward_from_this_round)\n",
    "    \n",
    "#     probEndingNow = tf.minimum(1, tf.maximum(0., envModelPredictedDone(predictedOuts)))\n",
    "#     probNotDone *= 1 - probEndingNow\n",
    "    \n",
    "    predicted_observations = bound_to_env_observation_space(env_model_predicted_observation(predicted_outs))\n",
    "    \n",
    "agent_loss = -tf.reduce_mean(total_predicted_reward)\n",
    "average_reward_per_action = tf.reduce_mean(tf.div(total_predicted_reward, EPISODE_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the envModel and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with InitializeNewVariables():\n",
    "    env_model_optimizer = tf.train.AdamOptimizer(.01).minimize(env_model_loss, var_list=tf.get_collection(\"env_model_vars\")) #BEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with InitializeNewVariables():\n",
    "    agent_optimizer = tf.train.AdamOptimizer(.1).minimize(agent_loss, var_list=tf.get_collection(\"agent_vars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run this after changing the agent's structure\n",
    "tf.initialize_variables(tf.get_collection(\"agent_vars\")).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SimAnn optimizer like thing\n",
    "\n",
    "collection_to_optimize = tf.get_collection(\"agent_vars\")\n",
    "# value_to_minimize = agent_loss\n",
    "\n",
    "# current_value = value_to_minimize.eval()\n",
    "current_value = generate_data(epsilon=0., num_episodes=1, episode_len=10, feed_dict=feed_dict, render=False)\n",
    "print \"current_value:\", current_value\n",
    "\n",
    "for iteration in xrange(5):\n",
    "    feed_dict = {}\n",
    "\n",
    "    for tensor in collection_to_optimize:\n",
    "        feed_dict[tensor] = tf.add(tensor, tf.random_normal(tf.shape(tensor))).eval()\n",
    "\n",
    "#     new_value = value_to_minimize.eval(feed_dict=feed_dict)\n",
    "    new_value = generate_data(epsilon=0., num_episodes=1, feed_dict=feed_dict, render=False)\n",
    "    print \"average_reward_per_action: \", average_reward_per_action.eval()\n",
    "    \n",
    "    if new_value < current_value:\n",
    "        for var in feed_dict:\n",
    "            var.assign(feed_dict[var]).eval()\n",
    "\n",
    "        current_value = new_value\n",
    "        print \"average_reward_per_action: \", average_reward_per_action.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Training on \", len(initial_observations), \" episodes\"\n",
    "\n",
    "env_model_ins_data = np.concatenate((np.asarray(old_observations), np.asarray(actions)), 1)\n",
    "print \"env_model_ins_data.shape:\", env_model_ins_data.shape\n",
    "\n",
    "env_model_outs_data = np.concatenate((np.asarray(observations), np.asarray([rewards]).T, np.asarray([dones]).T), 1)\n",
    "print \"env_model_outs_data.shape:\", env_model_outs_data.shape\n",
    "\n",
    "print \"Training the environment model on \" + str(env_model_ins_data.shape[0]) + \" observations\"\n",
    "print\n",
    "\n",
    "AGENT_MINI_BATCH_SIZE = len(initial_observations) / 2\n",
    "print \"AGENT_MINI_BATCH_SIZE:\", AGENT_MINI_BATCH_SIZE\n",
    "ENV_MODEL_MINI_BATCH_SIZE = 1000\n",
    "print \"ENV_MODEL_MINI_BATCH_SIZE:\", ENV_MODEL_MINI_BATCH_SIZE\n",
    "\n",
    "for iteration in xrange(100):\n",
    "    #Train the env:\n",
    "    for i in xrange(3):\n",
    "        mini_batch_indices = np.random.randint(env_model_ins_data.shape[0], size=ENV_MODEL_MINI_BATCH_SIZE)\n",
    "\n",
    "        env_model_optimizer.run(feed_dict={\n",
    "            env_model_ins: env_model_ins_data[mini_batch_indices, :],\n",
    "            env_model_outs: env_model_outs_data[mini_batch_indices, :]\n",
    "        })\n",
    "    \n",
    "    if iteration % 10 == 0:\n",
    "        #Evaluate the env:\n",
    "        print \"new env_model_loss:\", env_model_loss.eval(feed_dict={\n",
    "            env_model_ins: env_model_ins_data,\n",
    "            env_model_outs: env_model_outs_data\n",
    "        })\n",
    "    \n",
    "        #Evaluate the agent:\n",
    "        print \"average_reward_per_action: \", average_reward_per_action.eval()\n",
    "    \n",
    "    #Train the agent:\n",
    "    mini_batch_indices = np.random.randint(len(initial_observations), size=AGENT_MINI_BATCH_SIZE)\n",
    "    \n",
    "    agent_optimizer.run(feed_dict={\n",
    "        predicted_observations: np.asarray(initial_observations)[mini_batch_indices, :]\n",
    "    })\n",
    "\n",
    "    if iteration % 10 == 0:\n",
    "        #Evaluate the agent:\n",
    "        print \"average_reward_per_action: \", average_reward_per_action.eval()\n",
    "    \n",
    "print\n",
    "print \"average_reward_per_action: \", average_reward_per_action.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_data(epsilon=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saver (not working)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saver = tf.train.Saver(envModelWs + envModelbs)\n",
    "saver.save(sess, \"envModelVars\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "saver.restore(sess, \"envModelVars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to run envs in parallel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "NUM_PARALLEL_ENVS = 10\n",
    "\n",
    "myObservations = []\n",
    "envs = [gym.make(\"Pendulum-v0\") for _ in xrange(NUM_PARALLEL_ENVS)]\n",
    "\n",
    "for timestep in xrange(5):\n",
    "    for env in envs:\n",
    "        observation = env.reset()\n",
    "        observation = observation.astype(np.float32)\n",
    "        initialObservations.append(observation)\n",
    "        myObservations.append(observation)\n",
    "    \n",
    "    myActions = agent(np.asarray(myObservations)).eval()\n",
    "        \n",
    "    for action, env, observation in zip(tf.split(0, NUM_PARALLEL_ENVS, myActions), envs, myObservations):\n",
    "        oldObservation = observation\n",
    "        \n",
    "        observation, reward, done, info = env.step(action.eval()[0])\n",
    "        print action.eval()[0]\n",
    "        observation = observation.astype(np.float32)\n",
    "        \n",
    "        oldObservations.append(oldObservation)\n",
    "        actions.append(action)\n",
    "        observations.append(observation)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(timestep+1))\n",
    "            break\n",
    "            \n",
    "print \"We now have \" + str(len(observations)) + \" observations in total.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test long-term errors in env_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state =  env.reset()\n",
    "print \"initial:\", state\n",
    "\n",
    "print \"Real:\", env.step(np.asarray([0]))\n",
    "\n",
    "state = env_model(\n",
    "    np.reshape(\n",
    "        np.concatenate(\n",
    "            (starting_state.astype(np.float32), np.asarray([0.]).astype(np.float32)), 0\n",
    "        ), (1, 4)\n",
    "    )\n",
    ").eval()\n",
    "print \"predicted:\", state\n",
    "\n",
    "\n",
    "for i in xrange(5):\n",
    "    print \"real:\", env.step(np.asarray([0]))\n",
    "\n",
    "    state = env_model(\n",
    "        np.reshape(\n",
    "            np.concatenate(\n",
    "                (state[0][:3], np.asarray([0.]).astype(np.float32)), 0\n",
    "            ), (1, 4)\n",
    "        )\n",
    "    ).eval()\n",
    "    print \"predicted:\", state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
