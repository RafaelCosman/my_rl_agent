{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Workout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "NOW\n",
    "- Try the simann approach\n",
    "\n",
    "LATER\n",
    "- Penalize the env for long-term differences\n",
    "\n",
    "DONE\n",
    "- Make the agent training stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import random\n",
    "import datetime\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use(\"Pdf\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the session. An interactive session is a session that is automatically your default session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own little library of helper functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.569  0.551]]\n",
      "[[ 0.069  0.092]]\n"
     ]
    }
   ],
   "source": [
    "def fc(input_tensor, in_size, out_size, collection=None, non_linearity=tf.nn.sigmoid):\n",
    "    \n",
    "    collections = [tf.GraphKeys.VARIABLES, tf.GraphKeys.TRAINABLE_VARIABLES]\n",
    "    if collection is not None:\n",
    "        collections.append(collection)\n",
    "        \n",
    "    W = tf.get_variable(\n",
    "        name=\"W\",\n",
    "        initializer=tf.truncated_normal(\n",
    "            shape=[in_size, out_size],\n",
    "            mean=0.0,\n",
    "            stddev=0.1\n",
    "        ),\n",
    "        collections=collections\n",
    "    )\n",
    "    \n",
    "    b = tf.get_variable(\n",
    "        name=\"b\",\n",
    "        initializer=tf.constant(\n",
    "            value=0.1,\n",
    "            shape=[out_size]\n",
    "        ),\n",
    "        collections=collections\n",
    "    )\n",
    "    \n",
    "    return non_linearity( tf.matmul(input_tensor, W) + b )\n",
    "\n",
    "# fc test\n",
    "fc_test_template = tf.make_template(\"fc_test_template\", fc, in_size=2, out_size=2)\n",
    "op = fc_test_template([[0., 1.]])\n",
    "tf.initialize_all_variables().run()\n",
    "print op.eval()\n",
    "\n",
    "def fc_stack(input_tensor, list_of_sizes, collection=None):\n",
    "    result = input_tensor\n",
    "    \n",
    "    for layer_index in xrange(len(list_of_sizes)-1):\n",
    "\n",
    "        with tf.variable_scope(\"layer\"+str(layer_index)):\n",
    "        \n",
    "            if layer_index == len(list_of_sizes)-2:\n",
    "                non_linearity = tf.identity\n",
    "            else:\n",
    "                non_linearity = tf.nn.relu\n",
    "        \n",
    "            result = fc(\n",
    "                result,        \n",
    "                in_size=list_of_sizes[layer_index],\n",
    "                out_size=list_of_sizes[layer_index+1],\n",
    "                collection=collection,\n",
    "                non_linearity=non_linearity\n",
    "            )\n",
    "        \n",
    "    return result\n",
    "\n",
    "# fc_stack test\n",
    "fc_stack_test_template = tf.make_template(\"fc_stack_test_template\", fc_stack, list_of_sizes=[2, 3, 2])\n",
    "op = fc_stack_test_template([[0., 1.]])\n",
    "tf.initialize_all_variables().run()\n",
    "print op.eval()\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def one_hot(index, name=\"one_hot\"):\n",
    "    return tf.one_hot(indices=index, depth=env.observation_space.n, on_value=1, off_value=0, axis=None, name=name).eval()\n",
    "\n",
    "def getFeedDict(observation):\n",
    "    if type(env.observation_space) == gym.spaces.discrete.Discrete:\n",
    "        return {x:[one_hot(observation)]}\n",
    "    elif type(env.observation_space) == gym.spaces.box.Box:\n",
    "        return {x:[observation]}\n",
    "    else:\n",
    "        print \"ERR\"\n",
    "        \n",
    "class InitializeNewVariables:\n",
    "    def __enter__(self):\n",
    "        self.temp = set(tf.all_variables())\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        sess.run(tf.initialize_variables(set(tf.all_variables()) - self.temp))\n",
    "\n",
    "        \n",
    "### OpenAI Gym specific stuff ###\n",
    "def bound_to_env_action_space(my_action):\n",
    "    my_action = tf.maximum(my_action, env.action_space.low)\n",
    "    my_action = tf.minimum(my_action, env.action_space.high)\n",
    "    return my_action\n",
    "    \n",
    "def bound_to_env_observation_space(my_observation):\n",
    "    my_observation = tf.maximum(my_observation, env.observation_space.low)\n",
    "    my_observation = tf.minimum(my_observation, env.observation_space.high)\n",
    "    return my_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-06-09 16:22:51,504] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(1,)\n",
      "Box(3,)\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('ConfirmationBiasEasy-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('TwoRoundNondeterministicReward-v0')\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "print(env.action_space)\n",
    "ACTION_DIMS = env.action_space.shape[0]\n",
    "print(env.observation_space)\n",
    "OBSERVATION_DIMS = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some random data with which to build our env model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_observations = []\n",
    "actions = []\n",
    "observations = []\n",
    "rewards = []\n",
    "dones = []\n",
    "initial_observations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the actual simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_data(epsilon=0.0, num_episodes=2, episode_len=200, render=True, feed_dict={}):\n",
    "    total_reward = 0\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        observation = observation.astype(np.float32)\n",
    "        initial_observations.append(observation)\n",
    "\n",
    "\n",
    "        for timestep in xrange(episode_len):\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample() #choose a random action\n",
    "            else:\n",
    "                feed_dict[agent_in] = np.reshape(observation, [1, 3])\n",
    "                action = bound_to_env_action_space(agent_out).eval(feed_dict)[0]\n",
    "\n",
    "            old_observation = observation\n",
    "\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            observation = observation.astype(np.float32)\n",
    "\n",
    "            old_observations.append(old_observation)\n",
    "            actions.append(action)\n",
    "            observations.append(observation)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(timestep+1))\n",
    "                break\n",
    "\n",
    "    print \"We now have \" + str(len(observations)) + \" observations in total.\"\n",
    "    \n",
    "    return total_reward / (num_episodes * episode_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 104664 observations in total.\n"
     ]
    }
   ],
   "source": [
    "generate_data(epsilon=1.0, num_episodes=100, render=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we make the env model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_model_predicted_outs: Tensor(\"env_model/env_model/layer1/Identity:0\", shape=(?, 5), dtype=float32)\n",
      "env_model_loss: Tensor(\"env_model/env_model_loss:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ENV_MODEL_OUT_DIMS = OBSERVATION_DIMS + 1 + 1\n",
    "ENV_MODEL_IN_DIMS = OBSERVATION_DIMS + ACTION_DIMS\n",
    "\n",
    "env_model = tf.make_template(\n",
    "    \"env_model\",\n",
    "    fc_stack,\n",
    "    list_of_sizes=[ENV_MODEL_IN_DIMS, 1000, ENV_MODEL_OUT_DIMS],\n",
    "    collection=\"env_model_vars\"\n",
    ")\n",
    "\n",
    "env_model_predicted_observation = tf.make_template(\n",
    "    'env_model_predicted_observation',\n",
    "    lambda env_model_predicted_outs: tf.slice(\n",
    "        env_model_predicted_outs,\n",
    "        begin=[0, 0],\n",
    "        size=[-1, OBSERVATION_DIMS],\n",
    "        name=\"env_model_predicted_observation\"\n",
    "    )\n",
    ")\n",
    "\n",
    "env_model_predicted_reward = tf.make_template(\n",
    "    'env_model_predicted_reward',\n",
    "    lambda env_model_predicted_outs: tf.slice(\n",
    "        env_model_predicted_outs,\n",
    "        begin=[0, OBSERVATION_DIMS],\n",
    "        size=[-1, 1],\n",
    "        name=\"env_model_predicted_reward\"\n",
    "    )    \n",
    ")\n",
    "\n",
    "env_model_predicted_done = tf.make_template(\n",
    "    'env_model_predicted_done',\n",
    "    lambda env_model_predicted_outs: tf.slice(\n",
    "        env_model_predicted_outs,\n",
    "        begin=[0, OBSERVATION_DIMS + 1],\n",
    "        size=[-1, 1],\n",
    "        name=\"env_model_predicted_done\"\n",
    "    )\n",
    ")\n",
    "\n",
    "with tf.name_scope('env_model'):\n",
    "    env_model_ins = tf.placeholder(dtype=tf.float32, shape=[None, ENV_MODEL_IN_DIMS], name='ins')\n",
    "    env_model_outs = tf.placeholder(dtype=tf.float32, shape=[None, ENV_MODEL_OUT_DIMS], name='outs')\n",
    "    env_model_predicted_outs = env_model(env_model_ins)\n",
    "    print \"env_model_predicted_outs:\", env_model_predicted_outs\n",
    "\n",
    "    env_model_loss = tf.reduce_mean(tf.square(tf.sub(env_model_predicted_outs, env_model_outs)), name=\"env_model_loss\")\n",
    "    print \"env_model_loss:\", env_model_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = tf.make_template(\n",
    "    \"agent\",\n",
    "    fc_stack,\n",
    "    list_of_sizes=[3, 100, 1],\n",
    "    collection=\"agent_vars\"\n",
    ")\n",
    "\n",
    "agent_in = tf.placeholder(dtype=tf.float32, shape=[None, OBSERVATION_DIMS])\n",
    "agent_out = agent(agent_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make giant stack of agent, env, agent, env, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intermediate_env_rewards = []\n",
    "intermediate_actions = []\n",
    "intermediate_observations = []\n",
    "\n",
    "with InitializeNewVariables():\n",
    "    prob_not_done = tf.Variable(tf.constant(1.), trainable=False, name=\"prob_not_done\")\n",
    "    total_predicted_reward = tf.Variable(tf.constant(0.), trainable=False, name=\"total_predicted_reward\")\n",
    "\n",
    "predicted_observations = tf.placeholder_with_default(\n",
    "    np.asarray(initial_observations),\n",
    "    [None, 3],\n",
    "    name=\"initial_predicted_observations\"\n",
    ")\n",
    "\n",
    "EPISODE_LENGTH = 200\n",
    "\n",
    "for round in xrange(EPISODE_LENGTH):\n",
    "    action = bound_to_env_action_space(agent(predicted_observations))\n",
    "    \n",
    "    predicted_outs = env_model(tf.concat(1, [predicted_observations, action]))\n",
    "    \n",
    "#     rewardFromThisRound = tf.mul(envModelPredictedReward(predictedOuts), probNotDone)\n",
    "    reward_from_this_round = env_model_predicted_reward(predicted_outs)\n",
    "    \n",
    "    intermediate_env_rewards.append(reward_from_this_round)\n",
    "    intermediate_actions.append(action)\n",
    "    intermediate_observations.append(predicted_observations)\n",
    "    \n",
    "    total_predicted_reward = tf.add(total_predicted_reward, reward_from_this_round)\n",
    "    \n",
    "#     probEndingNow = tf.minimum(1, tf.maximum(0., envModelPredictedDone(predictedOuts)))\n",
    "#     probNotDone *= 1 - probEndingNow\n",
    "    \n",
    "    predicted_observations = bound_to_env_observation_space(env_model_predicted_observation(predicted_outs))\n",
    "    \n",
    "agent_loss = -tf.reduce_mean(total_predicted_reward)\n",
    "average_reward_per_action = tf.reduce_mean(tf.div(total_predicted_reward, EPISODE_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the envModel and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with InitializeNewVariables():\n",
    "    env_model_optimizer = tf.train.AdamOptimizer(.01).minimize(env_model_loss, var_list=tf.get_collection(\"env_model_vars\")) #BEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with InitializeNewVariables():\n",
    "    agent_optimizer = tf.train.AdamOptimizer(.1).minimize(agent_loss, var_list=tf.get_collection(\"agent_vars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run this after changing the agent's structure\n",
    "tf.initialize_variables(tf.get_collection(\"agent_vars\")).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 24639 observations in total.\n",
      "current_value: -2.14774266865\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c770cf36eed7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#     new_value = value_to_minimize.eval(feed_dict=feed_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"average_reward_per_action: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_reward_per_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c12fb1a3b0c9>\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m(epsilon, num_episodes, episode_len, render, feed_dict)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_in\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbound_to_env_action_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mold_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \"\"\"\n\u001b[0;32m--> 502\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3332\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3333\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3334\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 340\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 564\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 637\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    638\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    642\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m--> 628\u001b[0;31m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#SimAnn optimizer like thing\n",
    "\n",
    "collection_to_optimize = tf.get_collection(\"agent_vars\")\n",
    "# value_to_minimize = agent_loss\n",
    "\n",
    "# current_value = value_to_minimize.eval()\n",
    "current_value = generate_data(epsilon=0., num_episodes=1, episode_len=10, feed_dict=feed_dict, render=False)\n",
    "print \"current_value:\", current_value\n",
    "\n",
    "for iteration in xrange(5):\n",
    "    feed_dict = {}\n",
    "\n",
    "    for tensor in collection_to_optimize:\n",
    "        feed_dict[tensor] = tf.add(tensor, tf.random_normal(tf.shape(tensor))).eval()\n",
    "\n",
    "#     new_value = value_to_minimize.eval(feed_dict=feed_dict)\n",
    "    new_value = generate_data(epsilon=0., num_episodes=1, feed_dict=feed_dict, render=False)\n",
    "    print \"average_reward_per_action: \", average_reward_per_action.eval()\n",
    "    \n",
    "    if new_value < current_value:\n",
    "        for var in feed_dict:\n",
    "            var.assign(feed_dict[var]).eval()\n",
    "\n",
    "        current_value = new_value\n",
    "        print \"average_reward_per_action: \", average_reward_per_action.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on  529  episodes\n",
      "env_model_ins_data.shape: (104664, 4)\n",
      "env_model_outs_data.shape: (104664, 5)\n",
      "Training the environment model on 104664 observations\n",
      "\n",
      "AGENT_MINI_BATCH_SIZE: 264\n",
      "ENV_MODEL_MINI_BATCH_SIZE: 1000\n",
      "new env_model_loss: 0.0140952\n",
      "average_reward_per_action:  -7.21167\n",
      "average_reward_per_action:  -7.21166\n",
      "new env_model_loss: 0.00919901\n",
      "average_reward_per_action:  -7.36295\n",
      "average_reward_per_action:  -7.36221\n",
      "new env_model_loss: 0.00673637\n",
      "average_reward_per_action:  -7.48454\n",
      "average_reward_per_action:  -7.48451\n",
      "new env_model_loss: 0.0053211\n",
      "average_reward_per_action:  -7.67507\n",
      "average_reward_per_action:  -7.67502\n",
      "new env_model_loss: 0.00437061\n",
      "average_reward_per_action:  -7.70916\n",
      "average_reward_per_action:  -7.70134\n",
      "new env_model_loss: 0.00381559\n",
      "average_reward_per_action:  -7.73608\n",
      "average_reward_per_action:  -7.72703\n",
      "new env_model_loss: 0.00332808\n",
      "average_reward_per_action:  -7.81378\n",
      "average_reward_per_action:  -7.82057\n",
      "new env_model_loss: 0.0030367\n",
      "average_reward_per_action:  -7.89392\n",
      "average_reward_per_action:  -7.89294\n",
      "new env_model_loss: 0.00274228\n",
      "average_reward_per_action:  -7.83783\n",
      "average_reward_per_action:  -7.83705\n",
      "new env_model_loss: 0.00255788\n",
      "average_reward_per_action:  -7.85639\n",
      "average_reward_per_action:  -7.85636\n",
      "\n",
      "average_reward_per_action:  -7.66347\n"
     ]
    }
   ],
   "source": [
    "print \"Training on \", len(initial_observations), \" episodes\"\n",
    "\n",
    "env_model_ins_data = np.concatenate((np.asarray(old_observations), np.asarray(actions)), 1)\n",
    "print \"env_model_ins_data.shape:\", env_model_ins_data.shape\n",
    "\n",
    "env_model_outs_data = np.concatenate((np.asarray(observations), np.asarray([rewards]).T, np.asarray([dones]).T), 1)\n",
    "print \"env_model_outs_data.shape:\", env_model_outs_data.shape\n",
    "\n",
    "print \"Training the environment model on \" + str(env_model_ins_data.shape[0]) + \" observations\"\n",
    "print\n",
    "\n",
    "AGENT_MINI_BATCH_SIZE = len(initial_observations) / 2\n",
    "print \"AGENT_MINI_BATCH_SIZE:\", AGENT_MINI_BATCH_SIZE\n",
    "ENV_MODEL_MINI_BATCH_SIZE = 1000\n",
    "print \"ENV_MODEL_MINI_BATCH_SIZE:\", ENV_MODEL_MINI_BATCH_SIZE\n",
    "\n",
    "for iteration in xrange(100):\n",
    "    #Train the env:\n",
    "    for i in xrange(3):\n",
    "        mini_batch_indices = np.random.randint(env_model_ins_data.shape[0], size=ENV_MODEL_MINI_BATCH_SIZE)\n",
    "\n",
    "        env_model_optimizer.run(feed_dict={\n",
    "            env_model_ins: env_model_ins_data[mini_batch_indices, :],\n",
    "            env_model_outs: env_model_outs_data[mini_batch_indices, :]\n",
    "        })\n",
    "    \n",
    "    if iteration % 10 == 0:\n",
    "        #Evaluate the env:\n",
    "        print \"new env_model_loss:\", env_model_loss.eval(feed_dict={\n",
    "            env_model_ins: env_model_ins_data,\n",
    "            env_model_outs: env_model_outs_data\n",
    "        })\n",
    "    \n",
    "        #Evaluate the agent:\n",
    "        print \"average_reward_per_action: \", average_reward_per_action.eval()\n",
    "    \n",
    "    #Train the agent:\n",
    "    mini_batch_indices = np.random.randint(len(initial_observations), size=AGENT_MINI_BATCH_SIZE)\n",
    "    \n",
    "    agent_optimizer.run(feed_dict={\n",
    "        predicted_observations: np.asarray(initial_observations)[mini_batch_indices, :]\n",
    "    })\n",
    "\n",
    "    if iteration % 10 == 0:\n",
    "        #Evaluate the agent:\n",
    "        print \"average_reward_per_action: \", average_reward_per_action.eval()\n",
    "    \n",
    "print\n",
    "print \"average_reward_per_action: \", average_reward_per_action.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_data(epsilon=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saver (not working)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saver = tf.train.Saver(envModelWs + envModelbs)\n",
    "saver.save(sess, \"envModelVars\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "saver.restore(sess, \"envModelVars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to run envs in parallel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "NUM_PARALLEL_ENVS = 10\n",
    "\n",
    "myObservations = []\n",
    "envs = [gym.make(\"Pendulum-v0\") for _ in xrange(NUM_PARALLEL_ENVS)]\n",
    "\n",
    "for timestep in xrange(5):\n",
    "    for env in envs:\n",
    "        observation = env.reset()\n",
    "        observation = observation.astype(np.float32)\n",
    "        initialObservations.append(observation)\n",
    "        myObservations.append(observation)\n",
    "    \n",
    "    myActions = agent(np.asarray(myObservations)).eval()\n",
    "        \n",
    "    for action, env, observation in zip(tf.split(0, NUM_PARALLEL_ENVS, myActions), envs, myObservations):\n",
    "        oldObservation = observation\n",
    "        \n",
    "        observation, reward, done, info = env.step(action.eval()[0])\n",
    "        print action.eval()[0]\n",
    "        observation = observation.astype(np.float32)\n",
    "        \n",
    "        oldObservations.append(oldObservation)\n",
    "        actions.append(action)\n",
    "        observations.append(observation)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(timestep+1))\n",
    "            break\n",
    "            \n",
    "print \"We now have \" + str(len(observations)) + \" observations in total.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test long-term errors in env_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial: [-0.738  0.675  0.994]\n",
      "\n",
      "Real: (array([-0.786,  0.618,  1.5  ]), -5.8631032467148243, False, {})\n",
      "predicted: [[-0.776  0.651  1.49  -5.829 -0.02 ]]\n",
      "\n",
      "real_state: (array([-0.843,  0.538,  1.963]), -6.3549570814529046, False, {})\n",
      "predicted: [[ -8.362e-01   5.667e-01   1.942e+00  -6.186e+00  -4.226e-03]]\n",
      "difference: -0.0149590012109\n",
      "\n",
      "real_state: (array([-0.901,  0.434,  2.366]), -7.0109559060898015, False, {})\n",
      "predicted: [[ -9.011e-01   4.473e-01   2.334e+00  -6.836e+00   6.481e-03]]\n",
      "difference: 0.0197284769727\n",
      "\n",
      "real_state: (array([-0.951,  0.309,  2.692]), -7.8085079072573649, False, {})\n",
      "predicted: [[ -9.581e-01   2.991e-01   2.641e+00  -7.733e+00   5.858e-03]]\n",
      "difference: 0.0683815018172\n",
      "\n",
      "real_state: (array([-0.986,  0.168,  2.924]), -8.7160739922585559, False, {})\n",
      "predicted: [[ -1.010e+00   1.492e-01   2.847e+00  -8.797e+00  -7.470e-03]]\n",
      "difference: 0.119728166439\n",
      "\n",
      "real_state: (array([-1.   ,  0.016,  3.05 ]), -9.6943561071263034, False, {})\n",
      "predicted: [[ -1.045e+00  -5.242e-03   2.946e+00  -9.837e+00  -1.503e-02]]\n",
      "difference: 0.16973583008\n",
      "\n",
      "real_state: (array([-0.991, -0.137,  3.062]), -10.699419270325752, False, {})\n",
      "predicted: [[ -1.055e+00  -1.577e-01   2.927e+00  -1.071e+01  -8.154e-03]]\n",
      "difference: 0.220086351932\n",
      "\n",
      "real_state: (array([-0.96 , -0.281,  2.959]), -9.9643790564100776, False, {})\n",
      "predicted: [[ -1.058e+00  -2.930e-01   2.789e+00  -1.010e+01  -1.510e-03]]\n",
      "difference: 0.280773006682\n",
      "\n",
      "real_state: (array([-0.912, -0.41 ,  2.748]), -9.0354901313788663, False, {})\n",
      "predicted: [[-1.067 -0.401  2.521 -9.467  0.013]]\n",
      "difference: 0.37340700662\n",
      "\n",
      "real_state: (array([-0.855, -0.518,  2.441]), -8.1489607385831722, False, {})\n",
      "predicted: [[-1.088 -0.478  2.137 -8.941  0.04 ]]\n",
      "difference: 0.495997552005\n",
      "\n",
      "real_state: (array([-0.798, -0.603,  2.052]), -7.340627903858584, False, {})\n",
      "predicted: [[-1.115 -0.534  1.678 -8.553  0.059]]\n",
      "difference: 0.621918368339\n",
      "\n",
      "real_state: (array([-0.747, -0.665,  1.6  ]), -6.6436267990027806, False, {})\n",
      "predicted: [[-1.142 -0.571  1.14  -8.311  0.071]]\n",
      "difference: 0.761613553078\n",
      "\n",
      "real_state: (array([-0.709, -0.705,  1.102]), -6.0857500317346114, False, {})\n",
      "predicted: [[-1.159 -0.552  0.514 -8.212  0.089]]\n",
      "difference: 0.884007327491\n",
      "\n",
      "real_state: (array([-0.689, -0.725,  0.573]), -5.6881356851688976, False, {})\n",
      "predicted: [[-1.163 -0.523 -0.069 -8.278  0.083]]\n",
      "difference: 0.914133940843\n",
      "\n",
      "real_state: (array([-0.688, -0.726,  0.029]), -5.4652684007492329, False, {})\n",
      "predicted: [[-1.165 -0.459 -0.59  -8.416  0.065]]\n",
      "difference: 0.830223665429\n",
      "\n",
      "real_state: (array([-0.706, -0.708, -0.515]), -5.4257127472501789, False, {})\n",
      "predicted: [[-1.194 -0.37  -0.988 -8.749  0.065]]\n",
      "difference: 0.622842698135\n",
      "\n",
      "real_state: (array([-0.742, -0.67 , -1.046]), -5.5728176238758564, False, {})\n",
      "predicted: [[-1.247 -0.275 -1.305 -9.339  0.061]]\n",
      "difference: 0.368000802235\n",
      "\n",
      "real_state: (array([-0.792, -0.611, -1.549]), -5.9047984535589038, False, {})\n",
      "predicted: [[ -1.303  -0.167  -1.558 -10.116   0.073]]\n",
      "difference: 0.0760464166811\n",
      "\n",
      "real_state: (array([-0.849, -0.528, -2.007]), -6.413978806409574, False, {})\n",
      "predicted: [[ -1.348  -0.054  -1.728 -10.982   0.084]]\n",
      "difference: -0.253330725275\n",
      "\n",
      "real_state: (array([-0.906, -0.423, -2.403]), -7.0854273601400282, False, {})\n",
      "predicted: [[ -1.401   0.08   -1.814 -11.697   0.093]]\n",
      "difference: -0.596449785936\n",
      "\n",
      "real_state: (array([-0.955, -0.296, -2.72 ]), -7.8956495406514229, False, {})\n",
      "predicted: [[ -1.453   0.204  -1.813 -11.92    0.113]]\n",
      "difference: -0.9090924064\n",
      "\n",
      "real_state: (array([-0.988, -0.153, -2.942]), -8.8122370895215774, False, {})\n",
      "predicted: [[ -1.517   0.327  -1.739 -11.741   0.139]]\n",
      "difference: -1.15336820301\n",
      "\n",
      "real_state: (array([ -1.000e+00,  -4.820e-04,  -3.056e+00]), -9.7952529394083605, False, {})\n",
      "predicted: [[ -1.605   0.453  -1.605 -11.553   0.15 ]]\n",
      "difference: -1.2988711259\n",
      "\n",
      "real_state: (array([-0.988,  0.152, -3.056]), -10.800528956792787, False, {})\n",
      "predicted: [[ -1.722   0.579  -1.428 -11.472   0.17 ]]\n",
      "difference: -1.32132452806\n",
      "\n",
      "real_state: (array([-0.955,  0.295, -2.943]), -9.8698096285984303, False, {})\n",
      "predicted: [[ -1.873   0.712  -1.215 -11.563   0.197]]\n",
      "difference: -1.22715658587\n",
      "\n",
      "real_state: (array([-0.907,  0.422, -2.721]), -8.9435586501429416, False, {})\n",
      "predicted: [[ -2.06    0.871  -0.974 -11.849   0.222]]\n",
      "difference: -1.04211473839\n",
      "\n",
      "real_state: (array([-0.849,  0.528, -2.405]), -8.0633102049090919, False, {})\n",
      "predicted: [[ -2.288   1.057  -0.731 -12.286   0.269]]\n",
      "difference: -0.764545198457\n",
      "\n",
      "real_state: (array([-0.792,  0.61 , -2.009]), -7.2647748229662685, False, {})\n",
      "predicted: [[ -2.56    1.295  -0.489 -12.913   0.32 ]]\n",
      "difference: -0.438332099957\n",
      "\n",
      "real_state: (array([-0.743,  0.67 , -1.552]), -6.5806498019320712, False, {})\n",
      "predicted: [[ -2.872   1.592  -0.249 -13.681   0.364]]\n",
      "difference: -0.0949883049545\n",
      "\n",
      "real_state: (array([-0.706,  0.708, -1.049]), -6.0380917415090849, False, {})\n",
      "predicted: [[ -3.219e+00   1.946e+00   7.523e-03  -1.459e+01   4.069e-01]]\n",
      "difference: 0.217897173152\n",
      "\n",
      "real_state: (array([-0.688,  0.726, -0.519]), -5.6575496310915936, False, {})\n",
      "predicted: [[ -3.602   2.328   0.312 -15.63    0.444]]\n",
      "difference: 0.480646157942\n",
      "\n",
      "real_state: (array([-0.689,  0.725,  0.026]), -5.4528723690386069, False, {})\n",
      "predicted: [[ -4.024   2.73    0.661 -16.881   0.498]]\n",
      "difference: 0.695267453813\n",
      "\n",
      "real_state: (array([-0.709,  0.705,  0.57 ]), -5.432073258843837, False, {})\n",
      "predicted: [[ -4.531   3.162   1.011 -18.387   0.565]]\n",
      "difference: 0.922728024662\n",
      "\n",
      "real_state: (array([-0.747,  0.665,  1.098]), -5.597993468933641, False, {})\n",
      "predicted: [[ -5.151   3.627   1.337 -20.294   0.661]]\n",
      "difference: 1.20302461037\n",
      "\n",
      "real_state: (array([-0.797,  0.603,  1.597]), -5.9483020647236877, False, {})\n",
      "predicted: [[ -5.888   4.145   1.621 -22.699   0.78 ]]\n",
      "difference: 1.52479092391\n",
      "\n",
      "real_state: (array([-0.855,  0.519,  2.05 ]), -6.4746582727851116, False, {})\n",
      "predicted: [[ -6.754   4.736   1.859 -25.595   0.913]]\n",
      "difference: 1.87234949442\n",
      "\n",
      "real_state: (array([-0.912,  0.411,  2.439]), -7.1613179798410735, False, {})\n",
      "predicted: [[ -7.753   5.421   2.057 -29.019   1.061]]\n",
      "difference: 2.21267793462\n",
      "\n",
      "real_state: (array([-0.959,  0.282,  2.747]), -7.9838795760804082, False, {})\n",
      "predicted: [[ -8.9     6.218   2.229 -32.966   1.226]]\n",
      "difference: 2.52254299043\n",
      "\n",
      "real_state: (array([-0.99 ,  0.138,  2.958]), -8.9090795342157261, False, {})\n",
      "predicted: [[-10.215   7.143   2.39  -37.491   1.412]]\n",
      "difference: 2.78754180748\n",
      "\n",
      "real_state: (array([-1.   , -0.015,  3.061]), -9.8963759379256988, False, {})\n",
      "predicted: [[-11.72    8.214   2.552 -42.671   1.621]]\n",
      "difference: 3.00044628683\n",
      "\n",
      "real_state: (array([-0.986, -0.167,  3.05 ]), -10.712596876148316, False, {})\n",
      "predicted: [[-13.443   9.445   2.727 -48.591   1.858]]\n",
      "difference: 3.16753691331\n",
      "\n",
      "real_state: (array([-0.951, -0.309,  2.925]), -9.7753445058594455, False, {})\n",
      "predicted: [[-15.417  10.855   2.927 -55.371   2.129]]\n",
      "difference: 3.30021572418\n",
      "\n",
      "real_state: (array([-0.901, -0.434,  2.694]), -8.8520948223485956, False, {})\n",
      "predicted: [[-17.684  12.467   3.162 -63.155   2.441]]\n",
      "difference: 3.41337838623\n",
      "\n",
      "real_state: (array([-0.844, -0.537,  2.368]), -7.9784934025430729, False, {})\n",
      "predicted: [[-20.281  14.309   3.434 -72.103   2.799]]\n",
      "difference: 3.52603857107\n",
      "\n",
      "real_state: (array([-0.787, -0.617,  1.966]), -7.1900902379013374, False, {})\n",
      "predicted: [[-23.251  16.412   3.742 -82.365   3.206]]\n",
      "difference: 3.65877224458\n",
      "\n",
      "real_state: (array([-0.738, -0.674,  1.503]), -6.5191184784178651, False, {})\n",
      "predicted: [[-26.647  18.815   4.09  -94.106   3.672]]\n",
      "difference: 3.8318406458\n",
      "\n",
      "real_state: (array([-0.704, -0.71 ,  0.997]), -5.9920869468607822, False, {})\n",
      "predicted: [[ -30.529   21.562    4.485 -107.536    4.204]]\n",
      "difference: 4.06487899851\n",
      "\n",
      "real_state: (array([-0.687, -0.727,  0.464]), -5.6287571991021794, False, {})\n",
      "predicted: [[ -34.968   24.703    4.935 -122.892    4.816]]\n",
      "difference: 4.38141481725\n",
      "\n",
      "real_state: (array([-0.69 , -0.724, -0.081]), -5.4423508409694445, False, {})\n",
      "predicted: [[ -40.044   28.296    5.447 -140.453    5.513]]\n",
      "difference: 4.80607896175\n",
      "\n",
      "real_state: (array([-0.712, -0.702, -0.624]), -5.4403371512995848, False, {})\n",
      "predicted: [[ -45.846   32.408    6.032 -160.528    6.311]]\n",
      "difference: 5.36817617621\n",
      "\n",
      "real_state: (array([-0.751, -0.66 , -1.15 ]), -5.6250497215484794, False, {})\n",
      "predicted: [[ -52.482   37.115    6.702 -183.475    7.222]]\n",
      "difference: 6.10284278242\n",
      "\n",
      "real_state: (array([-0.803, -0.596, -1.645]), -5.9936024984856893, False, {})\n",
      "predicted: [[ -60.07    42.5      7.473 -209.71     8.263]]\n",
      "difference: 7.05286772981\n",
      "\n",
      "real_state: (array([-0.861, -0.509, -2.092]), -6.5369755875778681, False, {})\n",
      "predicted: [[ -68.749   48.66     8.359 -239.711    9.455]]\n",
      "difference: 8.26842655643\n",
      "\n",
      "real_state: (array([-0.917, -0.399, -2.474]), -7.2385985197862208, False, {})\n",
      "predicted: [[ -78.677   55.706    9.377 -274.025   10.819]]\n",
      "difference: 9.80494175082\n",
      "\n",
      "real_state: (array([-0.963, -0.268, -2.773]), -8.0731598924396533, False, {})\n",
      "predicted: [[ -90.035   63.767   10.544 -313.278   12.38 ]]\n",
      "difference: 11.7204988178\n",
      "\n",
      "real_state: (array([-0.992, -0.122, -2.974]), -9.0065569009658848, False, {})\n",
      "predicted: [[-103.029   72.987   11.883 -358.186   14.165]]\n",
      "difference: 14.0706795048\n",
      "\n",
      "real_state: (array([-1.   ,  0.031, -3.065]), -9.9976787377080196, False, {})\n",
      "predicted: [[-117.893   83.536   13.416 -409.561   16.206]]\n",
      "difference: 16.907130857\n",
      "\n",
      "real_state: (array([-0.983,  0.182, -3.043]), -10.618412453654784, False, {})\n",
      "predicted: [[-134.898   95.603   15.172 -468.332   18.542]]\n",
      "difference: 20.2796588937\n",
      "\n",
      "real_state: (array([-0.947,  0.322, -2.906]), -9.681019262316843, False, {})\n",
      "predicted: [[-154.356  109.405   17.178 -535.572   21.214]]\n",
      "difference: 24.2423663995\n",
      "\n",
      "real_state: (array([-0.896,  0.445, -2.665]), -8.7611360198604871, False, {})\n",
      "predicted: [[-176.619  125.193   19.468 -612.515   24.272]]\n",
      "difference: 28.8419598784\n",
      "\n",
      "real_state: (array([-0.838,  0.546, -2.331]), -7.8945460459697987, False, {})\n",
      "predicted: [[-202.09   143.255   22.083 -700.555   27.769]]\n",
      "difference: 34.1291001555\n",
      "\n",
      "real_state: (array([-0.781,  0.624, -1.921]), -7.1166050326690007, False, {})\n",
      "predicted: [[-231.232  163.92    25.07  -801.285   31.77 ]]\n",
      "difference: 40.1641476999\n",
      "\n",
      "real_state: (array([-0.734,  0.679, -1.453]), -6.4590569948230563, False, {})\n",
      "predicted: [[-264.573  187.562   28.484 -916.532   36.349]]\n",
      "difference: 47.0194730169\n",
      "\n",
      "real_state: (array([-0.701,  0.713, -0.944]), -5.9477527348314698, False, {})\n",
      "predicted: [[ -302.72    214.613    32.392 -1048.384    41.591]]\n",
      "difference: 54.7829132332\n",
      "\n",
      "real_state: (array([-0.686,  0.727, -0.409]), -5.6017690776204114, False, {})\n",
      "predicted: [[ -346.366   245.565    36.866 -1199.242    47.588]]\n",
      "difference: 63.5670024664\n",
      "\n",
      "real_state: (array([-0.691,  0.722,  0.136]), -5.4337090148264533, False, {})\n",
      "predicted: [[ -396.304   280.978    41.987 -1371.844    54.45 ]]\n",
      "difference: 73.5057945565\n",
      "\n",
      "real_state: (array([-0.715,  0.699,  0.678]), -5.4505045359023887, False, {})\n",
      "predicted: [[ -453.441   321.497    47.849 -1569.328    62.302]]\n",
      "difference: 84.7568879098\n",
      "\n",
      "real_state: (array([-0.756,  0.654,  1.202]), -5.6539809162888837, False, {})\n",
      "predicted: [[ -518.815   367.857    54.556 -1795.283    71.285]]\n",
      "difference: 97.5027888282\n",
      "\n",
      "real_state: (array([-0.809,  0.588,  1.693]), -6.0406874056173478, False, {})\n",
      "predicted: [[ -593.614   420.899    62.231 -2053.813    81.564]]\n",
      "difference: 111.955582667\n",
      "\n",
      "real_state: (array([-0.867,  0.499,  2.134]), -6.600909943800958, False, {})\n",
      "predicted: [[ -679.196   481.589    71.014 -2349.614    93.325]]\n",
      "difference: 128.359466358\n",
      "\n",
      "real_state: (array([-0.922,  0.386,  2.508]), -7.3172387852482688, False, {})\n",
      "predicted: [[ -777.117   551.027    81.062 -2688.058   106.781]]\n",
      "difference: 146.998737689\n",
      "\n",
      "real_state: (array([-0.967,  0.254,  2.797]), -8.1634515805327954, False, {})\n",
      "predicted: [[ -889.153   630.476    92.559 -3075.296   122.177]]\n",
      "difference: 168.202076234\n",
      "\n",
      "real_state: (array([-0.994,  0.107,  2.988]), -9.1046243493897041, False, {})\n",
      "predicted: [[-1017.341   721.378   105.714 -3518.357   139.793]]\n",
      "difference: 192.350305022\n",
      "\n",
      "real_state: (array([-0.999, -0.046,  3.068]), -10.099115055950861, False, {})\n",
      "predicted: [[-1164.009   825.385   120.764 -4025.294   159.948]]\n",
      "difference: 219.883595992\n",
      "\n",
      "real_state: (array([-0.981, -0.196,  3.034]), -10.524064652820613, False, {})\n",
      "predicted: [[-1331.821   944.385   137.984 -4605.309   183.009]]\n",
      "difference: 251.308646736\n",
      "\n",
      "real_state: (array([-0.942, -0.335,  2.887]), -9.5868697906447373, False, {})\n",
      "predicted: [[-1523.825  1080.541   157.687 -5268.942   209.394]]\n",
      "difference: 287.206635051\n",
      "\n",
      "real_state: (array([-0.89 , -0.456,  2.635]), -8.6707196097309538, False, {})\n",
      "predicted: [[-1743.509  1236.325   180.23  -6028.248   239.583]]\n",
      "difference: 328.243311572\n",
      "\n",
      "real_state: (array([-0.832, -0.555,  2.293]), -7.8115034908039505, False, {})\n",
      "predicted: [[-1994.863  1414.568   206.022 -6897.017   274.125]]\n",
      "difference: 375.178394725\n",
      "\n",
      "real_state: (array([-0.776, -0.631,  1.876]), -7.0443494730283351, False, {})\n",
      "predicted: [[-2282.452  1618.507   235.533 -7891.025   313.645]]\n",
      "difference: 428.882031639\n",
      "\n",
      "real_state: (array([-0.73 , -0.683,  1.404]), -6.4004887969427937, False, {})\n",
      "predicted: [[-2611.501  1851.845   269.299 -9028.333   358.863]]\n",
      "difference: 490.348239445\n",
      "\n",
      "real_state: (array([-0.699, -0.715,  0.891]), -5.9051055057515374, False, {})\n",
      "predicted: [[ -2987.987   2118.823    307.931 -10329.599    410.6  ]]\n",
      "difference: 560.71052362\n",
      "\n",
      "real_state: (array([-0.686, -0.728,  0.355]), -5.5765953636469989, False, {})\n",
      "predicted: [[ -3418.748   2424.288    352.133 -11818.46     469.795]]\n",
      "difference: 641.267006192\n",
      "\n",
      "real_state: (array([-0.693, -0.721, -0.191]), -5.4269515708084199, False, {})\n",
      "predicted: [[ -3911.608   2773.79     402.708 -13521.955    537.524]]\n",
      "difference: 733.504720612\n",
      "\n",
      "real_state: (array([-0.719, -0.695, -0.732]), -5.4625749995400774, False, {})\n",
      "predicted: [[ -4475.521   3173.677    460.573 -15471.031    615.017]]\n",
      "difference: 839.125743612\n",
      "\n",
      "real_state: (array([-0.761, -0.649, -1.253]), -5.6847809630262436, False, {})\n",
      "predicted: [[ -5120.73    3631.213    526.78  -17701.102    703.682]]\n",
      "difference: 960.074832897\n",
      "\n",
      "real_state: (array([-0.814, -0.58 , -1.74 ]), -6.0895436525794917, False, {})\n",
      "predicted: [[ -5858.952   4154.708    602.531 -20252.664    805.129]]\n",
      "difference: 1098.57871043\n",
      "\n",
      "real_state: (array([-0.873, -0.488, -2.175]), -6.6664396101584567, False, {})\n",
      "predicted: [[ -6703.595   4753.672    689.202 -23172.045    921.199]]\n",
      "difference: 1257.1850039\n",
      "\n",
      "real_state: (array([-0.927, -0.374, -2.541]), -7.3972076385425369, False, {})\n",
      "predicted: [[ -7670.008   5438.984    788.37  -26512.285   1054.004]]\n",
      "difference: 1438.81157359\n",
      "\n",
      "real_state: (array([-0.971, -0.24 , -2.821]), -8.2547149717131862, False, {})\n",
      "predicted: [[ -8775.739   6223.093    901.834 -30334.098   1205.954]]\n",
      "difference: 1646.78041419\n",
      "\n",
      "real_state: (array([-0.996, -0.092, -3.001]), -9.2032366709480975, False, {})\n",
      "predicted: [[-10040.872   7120.238   1031.655 -34706.84    1379.81 ]]\n",
      "difference: 1884.89040151\n",
      "\n",
      "real_state: (array([-0.998,  0.061, -3.07 ]), -10.200638742957523, False, {})\n",
      "predicted: [[-11488.393   8146.717   1180.192 -39709.961   1578.73 ]]\n",
      "difference: 2157.47687838\n",
      "\n",
      "real_state: (array([-0.977,  0.211, -3.024]), -10.429585756793539, False, {})\n",
      "predicted: [[-13144.59    9321.18    1350.142 -45434.371   1806.325]]\n",
      "difference: 2469.47755205\n",
      "\n",
      "real_state: (array([-0.937,  0.349, -2.866]), -9.4929322746555354, False, {})\n",
      "predicted: [[-15039.551  10664.947   1544.593 -51983.992   2066.729]]\n",
      "difference: 2826.55636416\n",
      "\n",
      "real_state: (array([-0.884,  0.467, -2.604]), -8.5808829175773926, False, {})\n",
      "predicted: [[-17207.693  12202.442   1767.072 -59477.859   2364.675]]\n",
      "difference: 3235.15741318\n",
      "\n",
      "real_state: (array([-0.826,  0.564, -2.254]), -7.7294007016355142, False, {})\n",
      "predicted: [[-19688.404  13961.583   2021.624 -68052.062   2705.577]]\n",
      "difference: 3702.68078477\n",
      "\n",
      "real_state: (array([-0.771,  0.637, -1.831]), -6.9733531887798987, False, {})\n",
      "predicted: [[-22526.734  15974.331   2312.872 -77862.367   3095.619]]\n",
      "difference: 4237.56656751\n",
      "\n",
      "real_state: (array([-0.726,  0.688, -1.353]), -6.3434366082135094, False, {})\n"
     ]
    }
   ],
   "source": [
    "state =  env.reset()\n",
    "print \"initial:\", state\n",
    "\n",
    "print\n",
    "\n",
    "print \"Real:\", env.step(np.asarray([0]))\n",
    "\n",
    "state = env_model(\n",
    "    np.reshape(\n",
    "        np.concatenate(\n",
    "            (state.astype(np.float32), np.asarray([0.]).astype(np.float32)), 0\n",
    "        ), (1, 4)\n",
    "    )\n",
    ").eval()\n",
    "print \"predicted:\", state\n",
    "\n",
    "\n",
    "for i in xrange(200):\n",
    "    print\n",
    "    real_state = env.step(np.asarray([0]))\n",
    "    print \"real_state:\", real_state\n",
    "\n",
    "    state = env_model(\n",
    "        np.reshape(\n",
    "            np.concatenate(\n",
    "                (state[0][:3], np.asarray([0.]).astype(np.float32)), 0\n",
    "            ), (1, 4)\n",
    "        )\n",
    "    ).eval()\n",
    "    print \"predicted:\", state\n",
    "    \n",
    "    print \"difference:\", np.sum(real_state[0] - state[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
