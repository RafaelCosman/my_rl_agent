{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import random\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use(\"Pdf\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My own little library of helper functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, collections=None, name=None):\n",
    "    initial = tf.truncated_normal(shape, mean=0.0, stddev=0.1)\n",
    "    newVar = tf.Variable(initial, collections=collections, name=name)\n",
    "    print name + \":\", newVar\n",
    "    return newVar\n",
    "\n",
    "def bias_variable(shape, collections=None, name=None):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    newVar = tf.Variable(initial, collections=collections, name=name)\n",
    "    print name + \":\", newVar\n",
    "    return newVar\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def makePlaceholder(dtype=tf.float32, shape=None, name=None):\n",
    "    newPlaceholder = tf.placeholder(dtype, shape, name)\n",
    "    print name + \":\", newPlaceholder\n",
    "    return newPlaceholder\n",
    "\n",
    "# Example usage:\n",
    "# with tf.name_scope(\"agent\"):\n",
    "#     with tf.name_scope(\"layer1\"):\n",
    "#         action = fc(observation, 1)\n",
    "def fc(inputTensor, inNeurons, outNeurons, extraCollection=None):\n",
    "    \n",
    "    collections = [tf.GraphKeys.VARIABLES]\n",
    "    if extraCollection is not None:\n",
    "        collections.append(extraCollection)\n",
    "    \n",
    "    W = weight_variable([inNeurons, outNeurons], collections=collections, name=\"W\")\n",
    "    b = bias_variable([outNeurons], collections=collections, name=\"b\")\n",
    "    return tf.nn.relu( tf.matmul(inputTensor, W) + b )\n",
    "\n",
    "def fc_stack(inputTensor, listOfLayerSizes, extraCollection=None):\n",
    "    \n",
    "    intermediate = inputTensor\n",
    "    \n",
    "    for layerIndex in xrange(len(listOfLayerSizes) - 1):\n",
    "        with tf.name_scope(\"layer\" + str(layerIndex)):\n",
    "            intermediate = fc(intermediate, listOfLayerSizes[layerIndex], listOfLayerSizes[layerIndex+1], extraCollection)\n",
    "    \n",
    "    return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: Tensor(\"observation_10:0\", shape=(?, 3), dtype=float32)\n",
      "W: <tensorflow.python.ops.variables.Variable object at 0x11fb48d50>\n",
      "b: <tensorflow.python.ops.variables.Variable object at 0x11fb09090>\n",
      "W: <tensorflow.python.ops.variables.Variable object at 0x11fb69c90>\n",
      "b: <tensorflow.python.ops.variables.Variable object at 0x11a66ca90>\n"
     ]
    }
   ],
   "source": [
    "observation = makePlaceholder(shape=[None, 3], name=\"observation\")\n",
    "\n",
    "with tf.name_scope(\"agent\"):\n",
    "    action = fc_stack(observation, [3, 10, 1], extraCollection=\"agentVars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the session. An interactive session is a session that is automatically your default session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x11a1e41d0>> ignored\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-06-07 09:35:33,299] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(1,)\n",
      "Box(3,)\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('ConfirmationBiasEasy-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('TwoRoundNondeterministicReward-v0')\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot(index, name=\"one_hot\"):\n",
    "    return tf.one_hot(indices=index, depth=env.observation_space.n, on_value=1, off_value=0, axis=None, name=name).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeedDict(observation):\n",
    "    if type(env.observation_space) == gym.spaces.discrete.Discrete:\n",
    "        return {x:[one_hot(observation)]}\n",
    "    elif type(env.observation_space) == gym.spaces.box.Box:\n",
    "        return {x:[observation]}\n",
    "    else:\n",
    "        print \"ERR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some random data with which to build our env model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oldObservations = []\n",
    "actions = []\n",
    "observations = []\n",
    "rewards = []\n",
    "dones = []\n",
    "initialObservations = []\n",
    "\n",
    "for i_episode in range(50):\n",
    "    observation = env.reset()\n",
    "    observation = observation.astype(np.float32)\n",
    "    initialObservations.append(observation)\n",
    "    \n",
    "    for timestep in xrange(50):\n",
    "        env.render()\n",
    "        \n",
    "#         epsilon = 1.0\n",
    "#         if random.random() < epsilon:\n",
    "        action = env.action_space.sample() #choose a random action\n",
    "        \n",
    "        oldObservation = observation\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observation = observation.astype(np.float32)\n",
    "        \n",
    "        oldObservations.append(oldObservation)\n",
    "        actions.append(action)\n",
    "        observations.append(observation)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(timestep+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"\\n=== data ===\"\n",
    "envModelInsData = np.concatenate((np.asarray(oldObservations), np.asarray(actions)), 1)\n",
    "print \"envModelInsData.shape:\", envModelInsData.shape\n",
    "envModelOutsData = np.concatenate((np.asarray(observations), np.asarray([rewards]).T, np.asarray([dones]).T), 1)\n",
    "print \"envModelOutsData.shape:\", envModelOutsData.shape\n",
    "envModelInitialObservationData = np.asarray(initialObservations)\n",
    "print \"envModelInitialObservationData.shape:\", envModelInitialObservationData.shape\n",
    "\n",
    "print \"\\n=== placeholders ===\"\n",
    "envModelIns = makePlaceholder(shape=[None, envModelInsData.shape[1]], name=\"envModelIns\")\n",
    "envModelOuts = makePlaceholder(shape=[None, envModelOutsData.shape[1]], name=\"envModelOuts\")\n",
    "envModelInitialObservationOuts = makePlaceholder(shape=[None, 3], name=\"envModelInitialObservationOuts\")\n",
    "\n",
    "print \"\\n=== env variables ===\"\n",
    "envVariablesCollections = [tf.GraphKeys.VARIABLES, \"envVariables\"]\n",
    "\n",
    "layerSizes = [envModelInsData.shape[1], 10, 10, envModelOutsData.shape[1]]\n",
    "\n",
    "envModelWs = [weight_variable(\n",
    "    [layerSizes[i], layerSizes[i+1]],\n",
    "    collections=envVariablesCollections,\n",
    "    name=\"envModelW\" + str(i))\n",
    "              for i in xrange(3)]\n",
    "\n",
    "envModelbs = [bias_variable(\n",
    "    [layerSizes[i+1]],\n",
    "    collections=envVariablesCollections,\n",
    "    name=\"envModelb\" + str(i))\n",
    "              for i in xrange(3)]\n",
    "\n",
    "envModelInitialObservationb = bias_variable(\n",
    "    [envModelOutsData.shape[1] - 2],\n",
    "    collections=envVariablesCollections,\n",
    "    name=\"envModelInitialObservationb\")\n",
    "\n",
    "print \"\\n === env model internals ===\"\n",
    "def envModel(inputs=None):\n",
    "    if inputs is not None:\n",
    "        \n",
    "        intermediate = inputs\n",
    "        for i in xrange(3):\n",
    "            intermediate = tf.matmul(intermediate, envModelWs[i]) + envModelbs[i]\n",
    "            if i is not 2:\n",
    "                intermediate = tf.nn.relu(intermediate)\n",
    "                \n",
    "        envModelPredictedOuts = intermediate\n",
    "                \n",
    "        print \"envModelPredictedOuts:\", envModelPredictedOuts\n",
    "\n",
    "        # tf.slice(input_, begin, size, name=None)\n",
    "        return envModelPredictedOuts, \\\n",
    "                tf.slice(envModelPredictedOuts,\n",
    "                    begin=[0, 0],\n",
    "                    size=[-1, envModelOutsData.shape[1]-2],\n",
    "                    name=\"envModelPredictedDone\"), \\\n",
    "                tf.slice(envModelPredictedOuts,\n",
    "                   begin=[0, envModelOutsData.shape[1]-2],\n",
    "                   size=[-1, 1],\n",
    "                   name=\"envModelPredictedDone\"), \\\n",
    "                tf.slice(envModelPredictedOuts,\n",
    "                     begin=[0, envModelOutsData.shape[1]-1],\n",
    "                     size=[-1, 1],\n",
    "                     name=\"envModelPredictedDone\")\n",
    "\n",
    "        return envModelPredictedOuts, \\\n",
    "                envModelPredictedObservation, \\\n",
    "                envModelPredictedReward, \\\n",
    "                envModelPredictedDone\n",
    "    else:\n",
    "        return random.choice(initialObservations)\n",
    "            \n",
    "envModelPredictedOuts, \\\n",
    "    envModelPredictedObservation, \\\n",
    "    envModelPredictedReward, \\\n",
    "    envModelPredictedDone = envModel(envModelIns)\n",
    "\n",
    "envModelLoss = tf.reduce_mean(tf.square(tf.sub(envModelPredictedOuts, envModelOuts)))\n",
    "print \"envModelLoss:\", envModelLoss\n",
    "\n",
    "envModelPredictedInitialObservation = envModel()\n",
    "print \"envModelPredictedInitialObservation:\", envModelPredictedInitialObservation\n",
    "\n",
    "envModelInitialObservationLoss = tf.reduce_mean(tf.square(tf.sub(envModelPredictedInitialObservation, envModelInitialObservationOuts)))\n",
    "print \"envModelInitialObservationLoss:\", envModelInitialObservationLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probNotDone = tf.Variable(tf.constant(1.), trainable=False, name=\"probNotDone\")\n",
    "totalPredictedReward = tf.Variable(tf.constant(0.), trainable=False, name=\"totalPredictedReward\")\n",
    "\n",
    "envModelPredictedObservation = np.asarray(initialObservations)\n",
    "\n",
    "for round in xrange(50): # 200\n",
    "    action = getAction(envModelPredictedObservation)\n",
    "    \n",
    "    envModelPredictedOuts, envModelPredictedObservation, envModelPredictedReward, envModelPredictedDone = envModel(tf.concat(1, [envModelPredictedObservation, action]))\n",
    "    \n",
    "    rewardFromThisRound = tf.mul(envModelPredictedReward, probNotDone)\n",
    "    totalPredictedReward = tf.add(totalPredictedReward, rewardFromThisRound)\n",
    "    probNotDone *= 1 - envModelPredictedDone\n",
    "    \n",
    "agentLoss = -tf.reduce_mean(totalPredictedReward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually are going to run and train things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envModelOptimizer = {}\n",
    "# envModelOptimizer['AdagradOptimizer'] = tf.train.AdagradOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envVariables\"))\n",
    "# envModelOptimizer['AdadeltaOptimizer'] = tf.train.AdadeltaOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envVariables\"))\n",
    "# envModelOptimizer['GradientDescentOptimizer'] = tf.train.GradientDescentOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envVariables\"))\n",
    "# envModelOptimizer['MomentumOptimizer'] = tf.train.MomentumOptimizer(.01, momentum=0.1).minimize(envModelLoss, var_list=tf.get_collection(\"envVariables\"))\n",
    "envModelOptimizer['AdamOptimizer'] = tf.train.AdamOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envVariables\")) #BEST\n",
    "# envModelOptimizer['FtrlOptimizer'] = tf.train.FtrlOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envVariables\"))\n",
    "# envModelOptimizer['RMSPropOptimizer'] = tf.train.RMSPropOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envVariables\")) # GOOD\n",
    "\n",
    "agentOptimizer = {}\n",
    "agentOptimizer['AdagradOptimizer'] = tf.train.AdagradOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVariables\"))\n",
    "agentOptimizer['AdadeltaOptimizer'] = tf.train.AdadeltaOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVariables\"))\n",
    "agentOptimizer['GradientDescentOptimizer'] = tf.train.GradientDescentOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVariables\"))\n",
    "agentOptimizer['MomentumOptimizer'] = tf.train.MomentumOptimizer(.01, momentum=0.1).minimize(agentLoss, var_list=tf.get_collection(\"agentVariables\"))\n",
    "agentOptimizer['AdamOptimizer'] = tf.train.AdamOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVariables\")) #BEST\n",
    "agentOptimizer['FtrlOptimizer'] = tf.train.FtrlOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVariables\"))\n",
    "agentOptimizer['RMSPropOptimizer'] = tf.train.RMSPropOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVariables\")) # GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(3000):\n",
    "    miniBatchIndices = np.random.randint(envModelInsData.shape[0], size=100)\n",
    "    \n",
    "    envModelOptimizer['AdamOptimizer'].run(feed_dict={\n",
    "        envModelIns: envModelInsData[miniBatchIndices, :],\n",
    "        envModelOuts: envModelOutsData[miniBatchIndices, :]\n",
    "    })\n",
    "    \n",
    "    if i % 100 is 0:\n",
    "        print envModelLoss.eval(feed_dict={\n",
    "            envModelIns: envModelInsData,\n",
    "            envModelOuts: envModelOutsData\n",
    "        })\n",
    "\n",
    "# for i in xrange(20):\n",
    "#     tf.train.GradientDescentOptimizer(0.5).minimize(envModelInitialObservationLoss, var_list=tf.get_collection(\"envVariables\")).run(feed_dict={\n",
    "#         envModelInitialObservationOuts: envModelInitialObservationData \n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startingState = env.reset().astype(np.float32)\n",
    "print \"starting state:\", startingState\n",
    "print\n",
    "\n",
    "envModelPredictedOuts, envModelPredictedObservation, envModelPredictedReward, envModelPredictedDone = envModel(tf.expand_dims(tf.concat(0, [startingState, [0.]]), 0))\n",
    "\n",
    "print\n",
    "print \"predicted:\", envModelPredictedOuts.eval()\n",
    "actual = env.step(np.asarray([0]))\n",
    "observation, reward, done, info = actual\n",
    "print \"actual:\", actual\n",
    "# print envModelLoss.eval(feed_dict={\n",
    "#         envModelPredictedOuts: envModelPredictedOuts.eval(),\n",
    "#         envModelOuts: [list(observation) + [reward] + [done]]})\n",
    "\n",
    "tf.reduce_mean(tf.square(tf.sub(envModelPredictedOuts.eval(), [list(observation) + [reward] + [done]]))).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for iteration in xrange(300):\n",
    "    agentOptimizer['AdamOptimizer'].run()\n",
    "    \n",
    "    if iteration % 10 is 0:\n",
    "        print agentLoss.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "totalPredictedReward.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.train.GradientDescentOptimizer(.1).minimize(-totalPredictedReward, var_list=tf.get_collection(\"agentVariables\")).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(envModelWs + envModelbs)\n",
    "saver.save(sess, \"envModelVars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.restore(sess, \"envModelVars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    observation = observation.astype(np.float32)\n",
    "    \n",
    "    for timestep in xrange(50):\n",
    "        env.render()\n",
    "        \n",
    "#         action = env.action_space.sample() #choose a random action\n",
    "        print observation\n",
    "        action = getAction(tf.expand_dims(observation, 0)).eval()[0]\n",
    "        print action\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observation = observation.astype(np.float32)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(timestep+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oldObservations2 = []\n",
    "actions2 = []\n",
    "observations2 = []\n",
    "rewards2 = []\n",
    "dones2 = []\n",
    "initialObservations2 = []\n",
    "\n",
    "for i_episode in range(50):\n",
    "    observation = env.reset()\n",
    "    observation = observation.astype(np.float32)\n",
    "    initialObservations2.append(observation)\n",
    "    \n",
    "    for timestep in xrange(50):\n",
    "        env.render()\n",
    "        \n",
    "#         epsilon = 1.0\n",
    "#         if random.random() < epsilon:\n",
    "        action = 0#env.action_space.sample() #choose a random action\n",
    "        \n",
    "        oldObservation = observation\n",
    "        \n",
    "        observation, reward, done, info = env.step(np.asarray([0]))\n",
    "        observation = observation.astype(np.float32)\n",
    "        \n",
    "        oldObservations2.append(oldObservation)\n",
    "        actions2.append(action)\n",
    "        observations2.append(observation)\n",
    "        rewards2.append(reward)\n",
    "        dones2.append(done)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(timestep+1))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
