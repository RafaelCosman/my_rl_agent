{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import random\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use(\"Pdf\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the session. An interactive session is a session that is automatically your default session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My own little library of helper functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.074  0.164]]\n",
      "[[ 0.143  0.108]]\n"
     ]
    }
   ],
   "source": [
    "def fc(input_tensor, in_size, out_size, collection=None, non_linearity=tf.nn.relu):\n",
    "    \n",
    "    collections = [tf.GraphKeys.VARIABLES, tf.GraphKeys.TRAINABLE_VARIABLES]\n",
    "    if collection is not None:\n",
    "        collections.append(collection)\n",
    "        \n",
    "    W = tf.get_variable(\n",
    "        name=\"W\",\n",
    "        initializer=tf.truncated_normal(\n",
    "            shape=[in_size, out_size],\n",
    "            mean=0.0,\n",
    "            stddev=0.1\n",
    "        ),\n",
    "        collections=collections\n",
    "    )\n",
    "    \n",
    "    b = tf.get_variable(\n",
    "        name=\"b\",\n",
    "        initializer=tf.constant(\n",
    "            value=0.1,\n",
    "            shape=[out_size]\n",
    "        ),\n",
    "        collections=collections\n",
    "    )\n",
    "    \n",
    "    return non_linearity( tf.matmul(input_tensor, W) + b )\n",
    "\n",
    "# fc test\n",
    "fc_test_template = tf.make_template(\"fc_test_template\", fc, in_size=2, out_size=2)\n",
    "op = fc_test_template([[0., 1.]])\n",
    "tf.initialize_all_variables().run()\n",
    "print op.eval()\n",
    "\n",
    "def fc_stack(input_tensor, list_of_sizes, collection=None):\n",
    "    result = input_tensor\n",
    "    \n",
    "    for layer_index in xrange(len(list_of_sizes)-1):\n",
    "        \n",
    "        #I think this is the wrong way to do it:\n",
    "#         current_layer = tf.make_template(\n",
    "#             \"fc\"+str(layer_index),\n",
    "#             fc,\n",
    "#             in_size=list_of_sizes[layer_index],\n",
    "#             out_size=list_of_sizes[layer_index+1],\n",
    "#             collection=collection\n",
    "#         )\n",
    "        \n",
    "#         result = current_layer(result)\n",
    "\n",
    "        with tf.variable_scope(\"layer\"+str(layer_index)):\n",
    "        \n",
    "            if layer_index == len(list_of_sizes)-2:\n",
    "                non_linearity = tf.identity\n",
    "            else:\n",
    "                non_linearity = tf.nn.relu\n",
    "        \n",
    "            result = fc(\n",
    "                result,        \n",
    "                in_size=list_of_sizes[layer_index],\n",
    "                out_size=list_of_sizes[layer_index+1],\n",
    "                collection=collection,\n",
    "                non_linearity=non_linearity\n",
    "            )\n",
    "        \n",
    "    return result\n",
    "\n",
    "# fc_stack test\n",
    "fc_stack_test_template = tf.make_template(\"fc_stack_test_template\", fc_stack, list_of_sizes=[2, 3, 2])\n",
    "op = fc_stack_test_template([[0., 1.]])\n",
    "tf.initialize_all_variables().run()\n",
    "print op.eval()\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def makePlaceholder(dtype=tf.float32, shape=None, name=None):\n",
    "    newPlaceholder = tf.placeholder(dtype, shape, name)\n",
    "    print name + \":\", newPlaceholder\n",
    "    return newPlaceholder\n",
    "\n",
    "def one_hot(index, name=\"one_hot\"):\n",
    "    return tf.one_hot(indices=index, depth=env.observation_space.n, on_value=1, off_value=0, axis=None, name=name).eval()\n",
    "\n",
    "def getFeedDict(observation):\n",
    "    if type(env.observation_space) == gym.spaces.discrete.Discrete:\n",
    "        return {x:[one_hot(observation)]}\n",
    "    elif type(env.observation_space) == gym.spaces.box.Box:\n",
    "        return {x:[observation]}\n",
    "    else:\n",
    "        print \"ERR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-06-07 18:38:29,574] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(1,)\n",
      "Box(3,)\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('ConfirmationBiasEasy-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('TwoRoundNondeterministicReward-v0')\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some random data with which to build our env model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oldObservations = []\n",
    "actions = []\n",
    "observations = []\n",
    "rewards = []\n",
    "dones = []\n",
    "initialObservations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i_episode in range(50):\n",
    "    observation = env.reset()\n",
    "    observation = observation.astype(np.float32)\n",
    "    initialObservations.append(observation)\n",
    "    \n",
    "    for timestep in xrange(50):\n",
    "        env.render()\n",
    "        \n",
    "        action = env.action_space.sample() #choose a random action\n",
    "#         action = agent(tf.expand_dims(observation, 0)).eval()[0]\n",
    "        \n",
    "        oldObservation = observation\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observation = observation.astype(np.float32)\n",
    "        \n",
    "        oldObservations.append(oldObservation)\n",
    "        actions.append(action)\n",
    "        observations.append(observation)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(timestep+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "envModelInsData.shape: (2500, 4)\n",
      "envModelOutsData.shape: (2500, 5)\n",
      "ins: Tensor(\"envModel_1/ins:0\", shape=(?, 4), dtype=float32)\n",
      "outs: Tensor(\"envModel_1/outs:0\", shape=(?, 5), dtype=float32)\n",
      "envModelPredictedOuts: Tensor(\"envModel_1/envModel/layer2/Identity:0\", shape=(?, 5), dtype=float32)\n",
      "Loss: Tensor(\"envModel_1/Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('envModel'):\n",
    "    envModelInsData = np.concatenate((np.asarray(oldObservations), np.asarray(actions)), 1)\n",
    "    print \"envModelInsData.shape:\", envModelInsData.shape\n",
    "    \n",
    "    envModelOutsData = np.concatenate((np.asarray(observations), np.asarray([rewards]).T, np.asarray([dones]).T), 1)\n",
    "    print \"envModelOutsData.shape:\", envModelOutsData.shape\n",
    "    \n",
    "\n",
    "envModel = tf.make_template(\n",
    "    \"envModel\",\n",
    "    fc_stack,\n",
    "    list_of_sizes=[envModelInsData.shape[1], 10, 10, envModelOutsData.shape[1]],\n",
    "    collection=\"envModelVars\"\n",
    ")\n",
    "\n",
    "envModelPredictedObservation = tf.make_template(\n",
    "    'envModelPredictedObservation',\n",
    "    lambda envModelPredictedOuts: tf.slice(\n",
    "        envModelPredictedOuts,\n",
    "        begin=[0, 0],\n",
    "        size=[-1, envModelOutsData.shape[1]-2],\n",
    "        name=\"envModelPredictedDone\"\n",
    "    )\n",
    ")\n",
    "\n",
    "envModelPredictedReward = tf.make_template(\n",
    "    'envModelPredictedReward',\n",
    "    lambda envModelPredictedOuts: tf.slice(\n",
    "        envModelPredictedOuts,\n",
    "        begin=[0, envModelOutsData.shape[1]-2],\n",
    "        size=[-1, 1],\n",
    "        name=\"envModelPredictedDone\"\n",
    "    )    \n",
    ")\n",
    "\n",
    "envModelPredictedDone = tf.make_template(\n",
    "    'envModelPredictedDone',\n",
    "    lambda envModelPredictedOuts: tf.slice(\n",
    "        envModelPredictedOuts,\n",
    "        begin=[0, envModelOutsData.shape[1]-1],\n",
    "        size=[-1, 1],\n",
    "        name=\"envModelPredictedDone\"\n",
    "    )\n",
    ")\n",
    "\n",
    "with tf.name_scope('envModel'):\n",
    "    envModelIns = makePlaceholder(shape=[None, envModelInsData.shape[1]], name='ins')\n",
    "    envModelOuts = makePlaceholder(shape=[None, envModelOutsData.shape[1]], name='outs')\n",
    "    envModelPredictedOuts = envModel(envModelIns)\n",
    "    print \"envModelPredictedOuts:\", envModelPredictedOuts\n",
    "\n",
    "    envModelLoss = tf.reduce_mean(tf.square(tf.sub(envModelPredictedOuts, envModelOuts)))\n",
    "    print \"Loss:\", envModelLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = tf.make_template(\n",
    "    \"agent\",\n",
    "    fc_stack,\n",
    "    list_of_sizes=[3, 10, 10, 1],\n",
    "    collection=\"agentVars\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probNotDone = tf.Variable(tf.constant(1.), trainable=False, name=\"probNotDone\")\n",
    "totalPredictedReward = tf.Variable(tf.constant(0.), trainable=False, name=\"totalPredictedReward\")\n",
    "\n",
    "envModelPredictedObservation = np.asarray(initialObservations)\n",
    "\n",
    "for round in xrange(50): # 200\n",
    "    action = agent(envModelPredictedObservation)\n",
    "    \n",
    "    envModelPredictedOuts = envModel(tf.concat(1, [envModelPredictedObservation, action]))\n",
    "    \n",
    "    rewardFromThisRound = tf.mul(envModelPredictedReward(envModelPredictedOuts), probNotDone)\n",
    "    totalPredictedReward = tf.add(totalPredictedReward, rewardFromThisRound)\n",
    "    probNotDone *= 1 - envModelPredictedDone(envModelPredictedOuts)\n",
    "    \n",
    "agentLoss = -tf.reduce_mean(totalPredictedReward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually are going to run and train things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envModelOptimizer = {}\n",
    "# envModelOptimizer['AdagradOptimizer'] = tf.train.AdagradOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envModelVars\"))\n",
    "# envModelOptimizer['AdadeltaOptimizer'] = tf.train.AdadeltaOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envModelVars\"))\n",
    "# envModelOptimizer['GradientDescentOptimizer'] = tf.train.GradientDescentOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envModelVars\"))\n",
    "# envModelOptimizer['MomentumOptimizer'] = tf.train.MomentumOptimizer(.01, momentum=0.1).minimize(envModelLoss, var_list=tf.get_collection(\"envModelVars\"))\n",
    "envModelOptimizer['AdamOptimizer'] = tf.train.AdamOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envModelVars\")) #BEST\n",
    "# envModelOptimizer['FtrlOptimizer'] = tf.train.FtrlOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envModelVars\"))\n",
    "# envModelOptimizer['RMSPropOptimizer'] = tf.train.RMSPropOptimizer(.01).minimize(envModelLoss, var_list=tf.get_collection(\"envModelVars\")) # GOOD\n",
    "\n",
    "agentOptimizer = {}\n",
    "agentOptimizer['AdagradOptimizer'] = tf.train.AdagradOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVars\"))\n",
    "agentOptimizer['AdadeltaOptimizer'] = tf.train.AdadeltaOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVars\"))\n",
    "agentOptimizer['GradientDescentOptimizer'] = tf.train.GradientDescentOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVars\"))\n",
    "agentOptimizer['MomentumOptimizer'] = tf.train.MomentumOptimizer(.01, momentum=0.1).minimize(agentLoss, var_list=tf.get_collection(\"agentVars\"))\n",
    "agentOptimizer['AdamOptimizer'] = tf.train.AdamOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVars\")) #BEST\n",
    "agentOptimizer['FtrlOptimizer'] = tf.train.FtrlOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVars\"))\n",
    "agentOptimizer['RMSPropOptimizer'] = tf.train.RMSPropOptimizer(.01).minimize(agentLoss, var_list=tf.get_collection(\"agentVars\")) # GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the environment on 10100 observations\n",
      "envModelLoss: 0.0104267\n",
      "envModelLoss: 0.00661189\n",
      "envModelLoss: 0.00665769\n",
      "envModelLoss: 0.00580353\n",
      "envModelLoss: 0.00672594\n",
      "envModelLoss: 0.00753446\n",
      "envModelLoss: 0.00878808\n",
      "envModelLoss: 0.00742256\n",
      "envModelLoss: 0.00629922\n",
      "envModelLoss: 0.00627531\n",
      "envModelLoss: 0.00615674\n",
      "envModelLoss: 0.00612343\n",
      "envModelLoss: 0.00618922\n",
      "envModelLoss: 0.00591131\n",
      "envModelLoss: 0.00693076\n",
      "envModelLoss: 0.00839876\n",
      "envModelLoss: 0.00694833\n",
      "envModelLoss: 0.0101185\n",
      "envModelLoss: 0.00665616\n",
      "envModelLoss: 0.00583771\n",
      "envModelLoss: 0.00628071\n",
      "envModelLoss: 0.00652909\n",
      "envModelLoss: 0.00566679\n",
      "envModelLoss: 0.00566707\n",
      "envModelLoss: 0.00661901\n",
      "envModelLoss: 0.00576583\n",
      "envModelLoss: 0.00700021\n",
      "envModelLoss: 0.00661019\n",
      "envModelLoss: 0.00606961\n",
      "envModelLoss: 0.007385\n"
     ]
    }
   ],
   "source": [
    "print \"Training the environment model on \" + str(len(observations)) + \" observations\"\n",
    "\n",
    "for i in xrange(3000):\n",
    "    miniBatchIndices = np.random.randint(envModelInsData.shape[0], size=100)\n",
    "    \n",
    "    envModelOptimizer['AdamOptimizer'].run(feed_dict={\n",
    "        envModelIns: envModelInsData[miniBatchIndices, :],\n",
    "        envModelOuts: envModelOutsData[miniBatchIndices, :]\n",
    "    })\n",
    "    \n",
    "    if i % 100 is 0:\n",
    "        print \"envModelLoss:\", envModelLoss.eval(feed_dict={\n",
    "            envModelIns: envModelInsData,\n",
    "            envModelOuts: envModelOutsData\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.394\n",
      "126.924\n",
      "125.793\n",
      "123.738\n",
      "123.387\n",
      "123.016\n",
      "122.99\n",
      "122.941\n",
      "122.917\n",
      "122.901\n",
      "122.885\n",
      "122.861\n",
      "122.824\n",
      "122.787\n",
      "122.739\n",
      "122.665\n",
      "122.573\n",
      "122.481\n",
      "122.386\n",
      "122.293\n",
      "122.211\n",
      "122.141\n",
      "122.078\n",
      "122.025\n",
      "121.986\n",
      "121.961\n",
      "121.943\n",
      "121.92\n",
      "121.902\n",
      "121.882\n"
     ]
    }
   ],
   "source": [
    "for iteration in xrange(300):\n",
    "    agentOptimizer['AdamOptimizer'].run()\n",
    "    \n",
    "    if iteration % 10 is 0:\n",
    "        print agentLoss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-111.216]\n",
      " [ -68.236]\n",
      " [-183.337]\n",
      " [ -55.46 ]\n",
      " [-244.474]\n",
      " [-282.559]\n",
      " [  10.617]\n",
      " [ -46.66 ]\n",
      " [   3.585]\n",
      " [-354.991]\n",
      " [  -9.498]\n",
      " [-139.527]\n",
      " [   8.546]\n",
      " [-473.339]\n",
      " [ -37.082]\n",
      " [  -3.563]\n",
      " [-258.51 ]\n",
      " [-208.396]\n",
      " [-423.167]\n",
      " [ -42.212]\n",
      " [-100.861]\n",
      " [   9.822]\n",
      " [ -41.376]\n",
      " [-222.225]\n",
      " [   9.889]\n",
      " [  -3.838]\n",
      " [-416.09 ]\n",
      " [-154.02 ]\n",
      " [-301.721]\n",
      " [ -27.022]\n",
      " [-183.664]\n",
      " [ -52.556]\n",
      " [ -62.272]\n",
      " [ -65.923]\n",
      " [   6.224]\n",
      " [  -3.022]\n",
      " [-198.112]\n",
      " [ -43.505]\n",
      " [ -99.217]\n",
      " [-119.066]\n",
      " [ -28.63 ]\n",
      " [  -0.719]\n",
      " [-103.654]\n",
      " [-262.513]\n",
      " [ -55.13 ]\n",
      " [  -9.855]\n",
      " [-100.202]\n",
      " [ -58.303]\n",
      " [-446.443]\n",
      " [ -39.749]]\n"
     ]
    }
   ],
   "source": [
    "print totalPredictedReward.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.train.GradientDescentOptimizer(.1).minimize(-totalPredictedReward, var_list=tf.get_collection(\"agentVariables\")).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'envModelWs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1721048200bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvModelWs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menvModelbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"envModelVars\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'envModelWs' is not defined"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(envModelWs + envModelbs)\n",
    "saver.save(sess, \"envModelVars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.restore(sess, \"envModelVars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.968, -0.252, -0.478], dtype=float32),\n",
       " array([ 0.954, -0.298, -0.967], dtype=float32),\n",
       " array([ 0.93 , -0.369, -1.491], dtype=float32),\n",
       " array([ 0.887, -0.461, -2.034], dtype=float32),\n",
       " array([ 0.821, -0.571, -2.579], dtype=float32),\n",
       " array([ 0.718, -0.696, -3.221], dtype=float32),\n",
       " array([ 0.564, -0.826, -4.042], dtype=float32),\n",
       " array([ 0.344, -0.939, -4.962], dtype=float32),\n",
       " array([ 0.053, -0.999, -5.966], dtype=float32),\n",
       " array([-0.293, -0.956, -7.015], dtype=float32)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldObservations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
